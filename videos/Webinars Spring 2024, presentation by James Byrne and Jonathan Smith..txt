welcome everybody um it's early morning here in the States but like I know it's like end of the afternoon for many of you who are joining for um from Europe um this is the community service Dynamics modeling systems webinar series um my name is Arena over I am the um deputy director of csdms um it CSD M for those of you who don't know has been around for um about 15 years now it's an NSF so National Science Foundation uh funded program that um um promotes modeling in the earth service processes and so our facility um has been rallying the communicating educating people about sustainable software and fair software development um and building a bit of like cyber infrastructure tools to like enable people um um building their own like scientific software and so the team includes software engineers and I saw like Mark is for example is here too and one of the purposes of these CSD M Euro um webinars is to exchange idea with like other Earth service process modelers um that are EUR European based and uh are doing have similar programs and do similar things um and see see whether we can learn from each other and so Sam rallied the British Antarctic Survey cyber infrastructure team and um we're gonna hear from Jonathan and um Johnny Goes by Johnny um I I suspect um and from dips so I'll hand it over to S thank you yeah so this is basically these European things um came around because we've got a little bit of seed cor funding which is aimed at bringing communities together um and so as Arena says we're looking at um telling people about the csdms community over here and and vice versa telling the csdms community and broader you know what some of the exciting digital things going on in Europe are um thanks very much for us folk joining us so early in the morning um I know it's early particularly early especially if you go west west of Colorado um So yeah thank you very much for joining us um yeah we've got I've got one little bit of advertising to do before I hand over to James and Johnny um we have a we're co-organizing a workshop um with csdms in the UK in October time um and so I just wanted to say hold this date we haven't got any specific information in terms of what the content will be and and Logistics Etc um but we do have dates and that's the 28th to the 31st of octo October um and it'll be Workshop hosted in the Lake District which is the northwest of the UK or Northwest of England rather um and we'll be looking broadly at things around what what enables the next generation of environmental modeling um details a little bit fuzzy at the moment but watch the space over the next few months as we as we flesh out details there and it would be great to see some of you guys there and I can see some familiar um names in the audience that I know will be really interested in the workshop so that's great um yeah as Arena says we have James and Johnny from the British Antartica survey here today um they are going to be telling us about or to read their title developing Ai and research pipelines for operational use um towards digital Twins and I won't give any more by way of an introduction because they will go through all in their talk themselves so James and Johnny do you want to get your slides up and check we all work looking okay brilliant that's looking good so I'll hand over to you excellent thank you very much Sam and thanks csdms for having us along uh so uh I'm James burn uh I'm lead research software engineer at the British antartic survey uh and I'll just allow Jonathan do a quick intro as well hi I'm Jonathan Smith I'm a um principal research scientist who is co-leading the autonomous Marine operations planning side in the AI lab and just for those who of you who might not know the British Antarctic Survey is uh unsurprisingly the British effort towards polar research and operations so we cover both Antarctic and uh the Arctic as well but also anywhere that has an interesting cryospheric science and uh related scientific disciplines as well so um it's a very multi-disciplinary uh research organiz ization um so the the point of Us coming along here is really not to focus necessarily on modeling but to try and state where we are in the British Antarctic Survey and the natural environment research Council who are who are our administrative body um with respect to uh building uh pipelines towards digital twins so uh we're very much looking at the uh how we're applying software engineering practices in the PO research domains uh and we're aiming uh and what we're we're trying to describe what we're aiming for uh by employing the concept of digital twins so I I understand that Gordon Blair gave a a talk to the group very recently uh I know Gordon and he's definitely the person you want to be talking to when talking about digital twins because he will definitely make you enthusiastic for them so I'm not going to tread on his turf at all um so uh but what is really important is uh how we're looking at coupling the data and process driven approaches that exist within the organization and how software engineering helps us do that um so just to go through how we're going to cover that so uh we're looking at the stages from research through uh from research and experimentation through to operations and then following onto impact so this is very much how our strategic Vision um our strategic Vision within that and bass is structured is how do we make sure that research is delivering not only our operations but also impacts at the end of it so there's very much the the the stages from motivation uh through to research so in this case I'm going to talk about icet CIS forecasting um how we then use pipelines in order to apply that research into an operational domain then Johnny is going to go through the autonomous Marine operations planning infrastructure so that's uh really wrapping around pipelines um and then we'll start defining how we are uh approaching digital twins through standardized approaches across all of these different Endeavors um so starting off with motivation just be very start right at this start right at the start um that we we're obviously focused in the British Antarctic Survey um on studying the balance between ice and water and their interaction with other Global Climate systems so I don't think I need to explain to anyone here that uh the Antarctic and Arctic are very significant influences towards climate um and we need to understand this at a variety of different scales and across a variety of different domains um but in doing that we need to uh be looking at driving real world impact so uh we're not purely research Focus we very much integrate Research into the way we uh approach our planning and operations um we Aid decision making uh to optimize uh the use of our resources we're contributing to wider approaches environmentally and technologically uh we're influencing environmental uh and ecosystem conservation for our Partnerships uh with other organizations ations and we're very much looking at influencing policym and Reporting um this is not an exhaustive list but it shows that you know we we need to be considering things at quite high level uh at all times so I'm going to start by talking about iset so this is something that I'm personally involved with very heavily and I use it as a sort of case a case and point as to why we take the approaches that we do uh when thinking towards longer term goals like digital twins um so it's a very useful frame uh of reference because it very much started off as a research product so up the top there you'll see a very deep convolutional Network very common infrastructure called the convolu called the unet sorry um and we basically couple we use that in order to process uh historical CIS observations atmospheric data potentially other forms of data in order to produce CIS uh predictions at the output now this was done a few years ago some by some uh by an excellent research called Tom Anderson and he generated some research code but then we've really worked out how to then wrap that into the uh the funny little diagram you see on the right there with what I call the ice onion so this is very much taking research and what what could be considered a model and then wrapping it in a pipeline an infrastructure and an ecosystem and so that's what I'm going to go through here so um it's an example of enabling and lowering the barrier to ongoing research um whilst uh looking to deliver multilateral uh operational Integrations and to do that we've got some key characteristics that we imposed say imposed that we converted Tom's original um icet code base uh with so we first started off by uh turning it into a very a dedicated python Library so rather than it just being research codebase we very much formulated it into something that could be maintained and used uh very easily by other researchers and operations um we then Incorporated patterns and common approaches to software engineering that allowed us to um to utilize different AI ml backends that's further lowering the barrier for people who were doing research into these methods um and we also designed it in such a way that it would be very easy to uh fit into workflow system so this then lowers the barrier to operationalizing these types of this type of re research uh and finally we um we very much looked at the code base that had resulted from all of this and said well how can we start to decompose the common functionality out of it and then generalize it to other environmental forecasting use cases um so hopefully that will become a bit more obvious but one thing to note here is that whenever we're producing or whenever anyone produces research like this be it machine learning or otherwise they start to produce lots of digital assets assets that then really communicate the value of that of that product if you like um so icet Library can produce all kinds of analysis and some of them are displayed here comparisons with uh observational models or like uh metrics around um how it's performing um but these uh these tools that are embedded within it are not necessarily restricted to a single purpose um the purpose of the icet library um is very much around C ice forecasting so that offers us an opportunity to say well okay uh how can we in my software engineering lingo incorporate the Unix philosophy we can build a library that does one thing it does it extremely well but then it delivers all of the assets elsewhere and that's very much uh in keeping with the Unix philosophy which uh basically goes along two points which is make each thing do one thing well and then think that all outputs will become another things input um and that very much much like drove that Central bubble of the onion that I described earlier so in building out as as we layered up that functionality so you end up with a library in the middle and you'll see in the top left of this diagram uh the little Ice net library and it literally is just called Ice net but then you can build whole Suite of different um tools and approaches alongside it in order to then uh layer up the uh the automation the interaction with other things um some of these elements got some of these elements got generalized so uh there's there's things called the uh one which is work in progress is called the download toolbox which uh allows us to download all of the forms of data that we necessarily need uh in order to run this pipeline but that can then be used by other pipelines as well um also there's a model ensembl which deals with uh running very large ensembles or very small ensembles if you really want to um of of these models uh uh as well um and we're in the process of decomposing this so that it so that these uh tools can be used elsewhere so icet is still on a journey as you can see there's only a couple of generalized tools out of this but uh many more have been designed with this uh you know toar out and use it for something else uh in the future and you'll see why that's important as we go through um the other reason to think about wrapping that initial Ice net library in a pipeline layer explicitly um is because it facilitates us scaling out both our research and operations it allows many people to be working on things at the same time without having to reinvent the will um but it also allows us to automate things uh should be be feeding them between other systems and this is definitely where my software engineering hat comes on which you'll hear quite strongly and Jonathan will hopefully balance me out a little bit with some proper science later on um but we are very much minimizing the duplication of data and effort so on the left like we uh we have a consistent data store like fed by the download toolbox now uh that actually allows us to share uh the source data across many different uh runs and many different experiments that people might be doing um it also allows for a lot of configurability between these different environments so you'll see there that they're called in ephemeral environments it allows us to actually feed a config into a a kind of blind a blank environment as it were and to stump the config in there and then reproduce the results that were previously encountered um convers conversely you can just blow them away and minimize your usage and if you're running an hpcs and things like this which is very helpful um so the icet pipeline is very simple when drawn by this but uh drawn like this but it can be implemented in a number of different ways uh at the moment we just use bash for the environment setup which might seem pretty archaic but it works extremely well because it makes it extremely portable between hbcs um it uses the library so these pipelines are not actually part of the library itself it's it's another repository it's another thing that wraps the icet library which can then continue to progress through different releases and then the the the pipeline is slightly agnostic to whatever version of icet you're using uh as I said we can use ensembling tools and configuration management quite easily the output of the run so those digital assets I mentioned that are created by the library uh can be made quite easily um standards compliant so we actually uh cool tools that are baked into the library in order to associate things like CF convention or compliant metadata uh to the outputs provided the pipeline is successful in producing a a re a reasonable output and that actually drives logic further down the chain um and it's kind of implicit document as well so having these this separate layer of of a pipe data pipeline as it were or an implementation of how to use the system or the library sorry at its core kind of acts as a a form of implicit documentation which actually makes it quite easy to say to someone this is how you run a forecast because there's actually a script here that runs all of the the the library components that produce a forecast from end to end um so there's a lot of complexity that could you know you could say well why not put it all into the library but then it makes the library extremely difficult to sort of generalize or break apart or modularize uh and certainly it makes it a little bit more difficult to create these kind of ephemeral environments that can be spun up and destroyed very quickly um the other the other thing to note as well this very much this decoupling of the of the model the heart of any of these infrastructures um is very much um it very much allows us to look at automation so if you're talking about operationalizing your data pipelines in order to feed other systems then um you definitely want to be approaching a workflow management system and looking at triggering all of these things automatically and studying their outputs um so that gets me on to talking about infrastructures and why we might want to do that kind of crazy automation well as I say we're a very multidisciplinary organization um we do a lot of science around the polar region and that means there's a lot of heterogenous data uh in the mix uh there's a lot of different fields of study but people might want to create complex systems uh from from many outputs that exist within this uh within this landscape so the scale diversity and potential impact of uh coupling these systems together uh demands that we take some software engineering approach approaches to the uh integration of them and that definitely drives us towards standardization so you'll see a few things around here we do quite a lot of work in machine learning around sea ice and Iceberg tracking uh we have a lot of Earth observation pipelines and numerical simulations relating to uh I sheet modeling and uh an observation of wildlife from space for example you see Ice net on the top right there uh we also have remote sensor networks which very much another field of study that I'm interested in so things like aridium Communications from a variety of different uh experiments in the field are very important we're also looking quite a lot at data Improvement using various uh types of research uh so these can be if these are reusable libraries or you know have Exemplar pipelines they can be dropped in nice and easily uh and then we have static data sets and uh and and ship and uh remote vehicles that I'm definitely not going to tread on Johnny's uh uh talk in the moment so hopefully I've demonstrated that the only sustainable operational dat Environmental data science will employ some level of pipeline development uh to deliver the digital assets uh uh and impact responsibly and efficiently um I'm a software sustainability fellow I know Sam is as well uh and a big part of this is actually um you know advocating for approaches that are going to reduce the uh the impact of what the negative impact of what we're doing in environmental data science if you're an environmental data scientist is um so now moving on to infrastructure um so what why you might ask are we talking about infrastructure in a slightly different way to the pipelines themselves well it allows us to share the digital assets used and provided from systems consistently that's allowing people to undertake research do the operational integration uh it's a way of feeding systems that might then be responsible for furthering on those assets to other places um it's also a form of education and appropriation it's a way of like giving people uh trans um transparent access Fair access to the digital assets under the hood so you definitely want to be looking at infrastructure in a different vein to the pipelines themselves which can tend to be quite complex and uh heavily developed um so again we go back to the motivations of why we're doing this so once the pipeline assets are produced they're delivered to one of potentially many hosted infrastructures that we might have in the case of ice this could be uh to talk about where we might want to place new sensors or where in particular we collaborate with WWF on uh various uh elements of uh ecosystem conservation uh and uh there's a bit there about autonomous Marine operations planning that again I'm not going to touch on Johnny's Turf too much but this is where the digital assets get fed out into other infrastructures in order to help them do what they need to do um is as I say when you separate out into a separate uh when you separate the pipeline and the infrastructures from one another the the digital assets you produce within the pipelines then become your interface uh that uh between the two which means the infrastructure can develop very very fast and very heavily and be modularized in uh in great ways um as long as it just deals with the compliant data that you're producing from the pipelines and that layer of Separation really helps you to uh iterate quickly um so I won't talk tooo much more about that but um I will hand over to Johnny now oh sorry I've missed the slide there so these are some of the uh the infrastructure interfaces that we have sorry so you will have as I mentioned we have a collaboration with the government and unat and WWF we provide a variety of different forecasts we the infrastructure provides things like apis as well as uh user interfaces but also automated systems that uh then send alerts on to people um so now I will definitely hand over to Johnny there you go Johnny perfect thanks James so I'm going to talk briefly about autonomous Marine operations planning so what we've been doing in this work is we've got a Proviso to meet Net Zero carbon production by 2040 and this is under the natural environment research Council and bass is part of that Council so one of the main aspects of carbon for the whole of bass is 60% of the carbon is produced through ship operations any reduction that we can make in ship operations and think of ways to do things differently can actually lead to significant carbon savings so our work in autonomous Marine operations planning is looking at using these pipelines that James has discussed before to First dynamically mesh the digital environment and that's on the left left hand side represented by meshify and then to constrain how things like sea ice or ocean currents or winds or waves affects the response of the ship or or autonomous vessels so what we do is we take all these different um forecasts or Legacy data sets we mesh them to create a digital environment we then determine what the ship actually can then do in that digital environment before going to the next stage of the project where we do Eco and timef friendly polar navigation so you can think of this work very similar to Google Maps as you get in your car you now get a specified fuel usage route as well as the quickest route so we're taking characteristics of how our icebreaking vessel the S David atra responds in sea ice conditions and we can actually then incorporate this into a root planning scheme to then determine the fuel usage for this vessel between a start location and the destination but it doesn't just stop there so when we actually get in the ice things change a bit so you can consider the next stage of the project as something like Tesla's autopilot so you're changing lanes on a Motorway is constantly determining the risk of these um actions that you're taking so the other work that we're well underway and we'll look forward to share in the future is um ice routa so this is a probabilistic machine learning method um that can incorporate uncertainty in the CIS conditions and then uh determine risk aware routing at much high resolution and then once we've got this understanding of the fuel usage for these different Marine routs we can go to the next stage so it's taking a a level out and this is autonomous Marine planning so you can consider like Amazon delivery Logistics when you get a package delivered they've determined what the best uh van to put the package on is and then also in their delivery Logistics they now incorporate in the states a uh a left hand a right hand turn so they take a lot of right-and turns to minimize the risk of going across traffic so what we're doing here is we're using these digital environments we're using the polar route work that we've been doing and then putting it all into an AI Logistics planner so this is a istic based um planning scheme so if we go on to the next slide I'm just going to talk you through polar route uh first and then we'll go through a very brief background to the logistics Planet so we're leveraging all this environmental information to determine these carbon and fuel efficient roots and we're now um operationalizing this on the bridge of the S David atra so just for this little example here which is taken at the peninsula around Antarctica this Transit would take about 1 day 18 hours and by taking the e eco-friendly route you're increasing your Transit Time by 14 minutes but you're having a saving of almost 4% carbon and what does this equate to well it's the same as running a petrol powered car for almost two years and then that's an insane saving that you can make only by increasing your Transit time in 14 minutes and then using this work we can go on to the next stage so James if you switch to the next slide so scientists are submitting um ship Time by appli forms so we can leverage this information to then say can we organize the science being done into a better system to minimize the fuel usage so what we see on the left hand side is the Marine facility planner where scientists would submit their science proposals in the middle you'll see a ship time out application form um with some key information so this task for example is a cruise it requires a total of uh 60 plus days and goes from po serenus to point and then the next stage is to understand constraints about the the item that could be doing this task so for a ship it would be this is what the crew would need this is the equipment you would need a ship has to be operational for the whole time of the task and this is like giving very basic constraints uh as machine code that a a planner would be using as they're going through this decision so then once we've done that you can move on to the next slide so we can then add constraints and all these things and we can look into three different time scales so if we can if we're able to reconcile a good planning scheme we can look at ways that we could have improved in the past to minimize fuel usage we can look at ways that we can plan five years plus ahead and then also we can quickly plan at very short u time intervals this is called nowcasting so if something goes wrong hits the fan um a mechanical problem arises we can replan multiple um months ahead or even multiple years and this is only possible if we leverage the full understanding of environmental forecasting root planning and Marine Logistics planning and in the Marine Logistics planning we're basically leveraging the constraints that a marine planner would use and if you move to the final slide my section we we've just done a proof of concept where we've looked at the national oceanography Center or NOC um previous 5 Years From 2017 to 2023 this would normally take 3 weeks to manually plan this and our initial proof of concept this is early stages of this project we've already got fuel saving okay it's only. 3% but that equates to 60,000 PS worth in cost savings um and this is equivalent to 60,000 tons of fuel and it takes the Marine planner the autonomous AI system 3 minutes while it would take a marine planner 3 weeks so we're now giving these Marine planners decision support tools that they can quickly try new scenarios or test out new ideas so we're going to basically expand this going forward and um basically expand to the next stages so I'll pass back to James cool thank you very much Johnny so uh I just move on to the uh the last layer of the bubble or I say bubble sorry it's a bubble diagram but it's the last layer of the onion you'll be glad to hear so um so the end game for bass is to be looking at the idea of an Antarctic digital twin so I drew this this uh diagram a long time ago when we're trying to work out how the sir David Asen fit in with the idea of having an Antarctic digital twin uh with all of these different systems uh working together and I think this comes back to what Gordon was talking about in his talk uh In Praise of the arrow or when he was talking about impraise of the arrows it's the idea of all of these systems being able to talk to one another in so much as you can couple the data and process models together to ask some of the questions that you need to uh as an integrated question as it were um but in order to do that you can't build an infrastructure every time you can't build you shouldn't have to build a large middleware infrastructure every time you want to ask a new question um so this idea of having a a minimal middleware implementation basically requires data models process models operational models or to exhibit some standards when they are built uh so that you can quite easily come along and say I tell you what I need some information from over here and over here in order to ask some of the questions and this is really I think where Johnny's team and has been uh focusing their efforts um for the environment is obviously very difficult but um the recent program called the information management framework for environmental digital twins which is not a short title so we call it IMF um already looked into what are the governance and implementation uh structures that you need to think about in order to make things interoperable by default um with the with IET in mind I drew this diagram and really the only things that need to change are the catalog descriptions of what assets make up iset uh and the presentation of those two other components that you might want to integrate now that's a very simplistic way of looking at it but actually it doesn't that that really conforms to this thin middle approach to building complex systems um and it's also very uh important for consistency in building sustainable infrastructure uh because then you're minimizing the amount of overhead that you have to imp that you have to maintain in order to uh um in order to sustain that integration uh for whatever purpose it is which is very important if you're in if you've ever done any support work so um this is a work in progress and the IMF was very much looking at the top- down approach to how to do this but uh we're also very keen that there's a bottom up set of developments that are just looking at building something that conforms to this initial picture and then over time the emergence of digital twins in the environment will become more of a reality um so just to go through a notion of generalizing this so obviously I talked about icet there I kind of came up with this diagram which basically says that you would have a similar template across all of the uh components that might need to interoperate so the the only real points of look at on the left really is the the things that you're describing to other people so it might be the generalized components that they can incorporate into their own systems uh the data that they might want to use to ask questions and the assets within uh any given um component generalizing these uh kind of um implementations is also very important uh as I say for um for people being able to adopt things within their own system so in the case of IET we're looking at CIS concentration but should we want to come AC uh use these uh things like the ensembling tool for example have been successfully used in II emulation where we've got a numerical model uh a process based model that uh requires some level of emulation in order to be operationally uh give operationally expedited answers and this is something that we're looking at internally but you can do this across the board you can say okay at the infrastructure level we might want to have conformant interfaces that are consistent across different sets of predi predictive uh predictive systems or in the case of things like the the routing packages you might have a whole load of cartographic in for structure for displaying Roots um and this is where really where it comes from it's building things with that generalizability in mind uh and so this just brings me on to the very last sort of summary of uh of building our environmental digital research infrastructure um so it's building an infrastructure of the future so is very much building it a brick at a time so I won't mention as it's being recorded the name of the blocks that are on the right hand side there because I don't want to get into a trademark uh issue um but there are some clear principles that we should probably embody so common capabilities should be exploited and openly utilized uh so that's very much advocating for the open source approach uh we should foster the Community Development far and wide and efforts like this where we can come and talk to you and and hopefully you can connect with us are great for that because only by building communities are we really going to understand what's already available uh we need to make it easy to align to existing efforts from the outset so that is very much looking at the Strategic ideas that are coming top down uh but also then being willing to um align to them uh from the bottom up as well and hopefully therefore driving them uh we need to remember that our Environmental Research is often used in other context so operational infrastructure is a very important to B but you might also uh we might also need to just remember that there might be social impacts there might be social uses for what we're doing as well uh we're trying to uh Champion new environmental data science through the community as I said um and we should be uh willing to Showcase high impact uh solutions that can be reused elsewhere and promoting that reuse um so I'll stop there I've always got a nice onion at the end because I really like the aliums so um hopefully there's a couple of questions and please do f free to connect with Jonathan or myself on the emails there thank you very much I'll stop Shar thank you very much both that was that was really really interesting um have we got any questions from the audience I think you can unmute yourself or put your hands up uh or put your question in the chat if you'd rather do neither and I'll keep my eye on the chat to get us warmed up um I kind of feel like Bas is ahead of the game in a lot of Senses in this realm like what you were talking about there James seemed really welldeveloped uh and I'm kind of curious how much how much of that was developed out of necessity because for instance you have this you have ships you have autonomous things um that you need to root as as the example that Johnny gave there so how much of it was developed out of necessity and how much of it was um pre-planned so and I think what really my question here is um how how much of the thinking behind this was done beforehand and you sat down as an organization and thought we need to build this digital infrastructure we need to build this onion to use your analogy how much of it was done reactively like for instance you you build the the research bit the center of the onion and then realize that you need the next layer and then the next layer yeah it's a really good question thank you I think the I would love to say it was all pre-planned and orchestrated from a strategic level but I would be slightly disingenuous if I were to do so um I think there has been a bit of one of the benefits of working for bass and the natural environment research Council as a whole I think is that all of the organizations have this sort of mix between research and operations so you end up with with a very diverse pool of talent that is offering new opinions and I think it very much when we see uh the AI lab for example at Bass that actually gave rise to the partnership with the alen churing Institute who are the AI leaders in in the UK uh that gave rise to IET that created research but then it was allowing people to see it and promoting it uh that then sort of gave us the impetus for someone like me to come along and say well actually I work in it and I've worked in Industry uh if you really want to make that available and useful to people then you need to start wrapping it in uh you know nice friendly layers that people can come along and integrate with and I think so it it is it's always going to be a mixture of proactive and reactive but I think this really sort of points at Community is the thing that makes it work is is really just allowing people to input ideas and then seeing where they go I mean some things have been you know not I wouldn't say unmitigated disasters but certainly we've had a few Freds that have collapsed um but then we've had other Freds like the aot team who utilize the Ice net forecast where that research because of our need to get to Net Zero has just ballooned and Johnny I don't know if you want to come in there like because the team is growing very quickly yeah this um reactive side of things was probably the main driving Factor um as things are changing and as we've got these new targets to meet you have to have an infrastructure that can called it and there's talk of um scaling to the point by 2040 to have hundreds of autonomous vessels all operating together and the only way to do that is to actually have the infrastructure that can support it and just to pick up on that I think that's where there's a clear strategic Direction but the the the points in the road map are not like yet always fought out so that's what we do reactively I feel Brill thank you any questions from other folks I was curious about um the fact that you start presenting like multiple sources of data like AI uh generated predictions um and like then there's like end users that are really like trying to make decisions and doing policy with it right you're talking about like the connections to the world um WWF Etc and so I wondered like how difficult it is to communicate like sort of the uncertainty in that whole process and whether that's like acceptable to your like decision making like Partners yeah it's really I mean from a from a predictive point of view that's really Troublesome of course is how do you represent uncertainty and any of these types of systems I think the big thing for me is not being an environmental research per say researcher per se but rather than someone who builds those environmental infrastructures is to be very clear about what product is it it is that you're developing and what solution it is in the real world context of that that uncertainty so say for examp to put a case in point if we're going to develop an early warning alert system then the research associated with the logic behind that is public research so that you know they have a scientific publication that backs up the reasoning for having an alert based on it um but at that point you know you're as a technologist I'm handing over that to public research that says this is the method used you have to be sure that you want to use that method in order to inform say a community at the very end um I think that's really important as well is that we you know you're not removing your uh the scientific life cycle here you're like supplementing it with digital delivery um and you're not claiming that one because this is a danger of AI in particular is for it to say oh well we we built that's really fancy tool why don't you trust it and it's just kind of like well you have to prove it like it's not it's not an all or nothing yeah great any other questions I've got one for Johnny actually um so this this aop tool sounds like it would be more broadly useful than just to um Bas and my question is simply is it being used outside of Bas or the research Council or is it is it kind of internal only so the aim in the long run is to tie into the Marine facility planner which is currently being used by many different National um Antarctic research councils so I think the Australian group is using it the Norwegians and there's several inside the UK that are using it but at this stage we're only very early days um so in the long run it will be tied into a pre-existing system but we need to first get the proof of concept there and get it along the line brilliant thank you we have a question from Juan sorry for the pronunciation if that was incorrect oh no it's it's great thank you actually sorry for the pronunciation Dr bar bar okay I I wonder if how from your P perspective how do you see applying this architecture or a modification of of it to a coastal water quality monitoring program like a something related to to monitoring hydrodynamics and but that has several layers of of for example socioeconomic and socio ecological variables and also has to deal with this decision making like pipeline how how do you see it applied to it yeah it's a good it's a really good good question so I think um so we at in NK we have several different centers so we actually have a center for Ecology and hydrology who I think would be quite well placed with this but uh in terms of the specific implementation but actually there is a collective effort uh that called the environmental data service that is really looking at how do we make uh the heterogeneous data sources available almost as quickly as possible one might say um so you would use pipelines in that case in order to take data sets offer if there's any way of automating quality assurance and publication uh uh the publication process uh of a data set and making it available within that environmental data service you then build your infrastructures on top of that in order to provide data delivery uh or or particular use cases and then obviously well I say Obviously but then that availability of data through something like the environmental data service would then allow you to build the digital twins as it were that uh allow people to ask questions of these uh dis data sources so I think the the specific um yeah the specific hydrodynamic question I can't answer to but the the general consensus I think it's fair to say within neck is that actually we have to be building these types of pipelines and infrastructures in a generalized sense so that everyone can benefit hopefully that answers your question oh wonderful thank you thank you yes welcome and I think in a in a lot of Senses that's where strategic proactive planning kind of comes in because it's it it's quite a difficult task isn't it thinking about data coming from different research centers from different disciplines Etc um and I think we're probably only really at the start of that journey in thinking about you know what the environmental data service EDS could do in terms of you know providing data that could be used in digital Twi wins in that type of research infrastructure yeah definitely at the start of the journey I think the promising thing is that by having all of the centers involved in a project like that you're enabling you're bringing everyone into the conversation the worst thing is that you develop digital research infrastructure which is then bespoke to a individual Senter and then try and retrofit it somewhere else so at least it's acknowledged as like you say a strategic development is very important I suppose guess opening opening this out to not just the UK and our research centers um a lot of this would be then generally applicable across any group of organizations that that have data around the broad environment or social things um just because we're talking about UK research centers doesn't mean that a lot of this stuff isn't applicable elsewhere I think that's a really important Point actually um is that the idea of digital twins as SC rightly pointed out uh you know that comes from the engineering and built environment more than anywhere else um certainly as ex aeronautical engineer or involved with Aeronautical Engineering they were very commonly used but they were constrained uh so we've taken that influence and we should definitely be forwarding it on to other areas that might be interested in adopting these Technologies so openness is very important yeah definitely I I mean just listening to like how powerful um this technology could be and this the shipping planning like actually like triggered sort of this idea of like wow if you would do this in new Arctic it would like actually be used for like Arctic shipping and like potentially like for like vessel movements that like are strategic or like military or so like so I don't know it seemed that the the openness of like sort of the scientific development or the development for this for scientific purposes is a bit of a comp conflict in that um in that realm potentially um so yeah I don't know whether you've like heard anything about that or um gotten some other people to comment on that so at Bass we have to um adhere to the open source code which I think is which means that everyone can use it uh there's always that provisor that you can use it at your own risk and for your own operations but the the advantage that it has if you use say root planning software to save fuel no matter what it's being used to save fuel for it's still a benefit saving fuel so I it's a difficult moral on that one but yes if you build the technology people come to like use it yeah I think ultimately the benefit of being open and allowing everyone to to use it far exceeds the well hope would exceed the uh the the risk of being uh open because ultimately the risk yes would be from people who want to um get the leg up on on delivering services like for you know automated Marine planning but in reality they're they're going to be at the same stage as everyone else so as long as the support is given to people who want to adopt it for benevolent means then hopefully uh hopefully you're you're going to your impact story is going to a lot more positive yeah I suppose yeah if if stuff ends up being used for nefarious means for to give an example it's better for it to be open source right rather than being something proprietary used by is is one way of looking at it it's a really interesting topic isn't it prob video any question questions from anyone else if not then I would like to thank James and Johnny again that was that was really interesting I've I've jotted down lots of things um to follow up on so that's really good um great talk this is has been recorded so it will be online later for anyone to uh watch back um the next talk I believe I'm just checking the web page now is on the 26th of Mark uh 26th of March and it's going to be with Mark um what systems Services can do for you overview of csdms products um and then we have another two talks coming up in April on the 16th and the 29th and just for reference I'm going to post those quickly in the chat now um Irena did you have any closing comments no just to thank uh um James and Jonathan for like showing us and like sort of highlighting the like more infrastructure parts of the uh of the system too which is like something that the csds community always like Embraces that there's this like whole Machinery in the back end of science um that should deliver and helps like it needs a lot of thought too and so it's nice to see that that commonality is there across organizations and that we can learn from each other absolutely